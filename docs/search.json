[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to PyPotteryInk",
    "section": "",
    "text": "Archaeological drawings are fundamental tools for studying and sharing information about ancient artefacts. PyPotteryInk aims to streamline the digitisation process by automating the conversion of pencil drawings into publication-ready inked versions while maintaining the high standards required for archaeological documentation.",
    "crumbs": [
      "Getting Started",
      "Home"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Welcome to PyPotteryInk",
    "section": "Key Features",
    "text": "Key Features\nThe package provides a comprehensive solution for archaeological drawing digitisation:\n\nAutomated conversion of pencil drawings to inked versions\nSmart preprocessing to optimize image characteristics\nAdvanced stippling pattern control for precise shading\nBatch processing capabilities for large collections\nDetailed quality control and validation tools\nPublication-ready output formats\nUser-friendly interface powered by Gradio",
    "crumbs": [
      "Getting Started",
      "Home"
    ]
  },
  {
    "objectID": "version_history.html",
    "href": "version_history.html",
    "title": "Version History",
    "section": "",
    "text": "üöÄ A PyPotteryInk major release üöÄ\nKey Features:\n\nMulti-GPU Support: Added support for Apple Silicon (M1/M2/M3) GPUs via Metal Performance Shaders\nGradio Web Interface: New user-friendly web interface with multiple tabs:\n\nHardware Check: Verify system compatibility\nModel Diagnostics: Preview processing with different settings\nPreprocessing: Analyze and optimize images before processing\nBatch Processing: Process multiple images with progress tracking\n\nAutomated Installation: One-click installation scripts for all platforms\nImproved Compatibility: Fixed compatibility issues with newest packages\nStatistics Visualization: New detailed statistics analysis with:\n\nSummary tables showing all key metrics\nInteractive distribution plots\nHistogram and KDE visualizations\nBox plots for metric overview\n\nAdvanced Export Options (Experimental):\n\nElement extraction: Automatically identify and extract individual pottery elements\nSVG export: Convert processed images to scalable vector graphics for publication\n\n\nwith contributions from Francesco di Filippo and Enzo Cocca",
    "crumbs": [
      "Project Info",
      "Version History"
    ]
  },
  {
    "objectID": "version_history.html#latest-release",
    "href": "version_history.html#latest-release",
    "title": "Version History",
    "section": "",
    "text": "üöÄ A PyPotteryInk major release üöÄ\nKey Features:\n\nMulti-GPU Support: Added support for Apple Silicon (M1/M2/M3) GPUs via Metal Performance Shaders\nGradio Web Interface: New user-friendly web interface with multiple tabs:\n\nHardware Check: Verify system compatibility\nModel Diagnostics: Preview processing with different settings\nPreprocessing: Analyze and optimize images before processing\nBatch Processing: Process multiple images with progress tracking\n\nAutomated Installation: One-click installation scripts for all platforms\nImproved Compatibility: Fixed compatibility issues with newest packages\nStatistics Visualization: New detailed statistics analysis with:\n\nSummary tables showing all key metrics\nInteractive distribution plots\nHistogram and KDE visualizations\nBox plots for metric overview\n\nAdvanced Export Options (Experimental):\n\nElement extraction: Automatically identify and extract individual pottery elements\nSVG export: Convert processed images to scalable vector graphics for publication\n\n\nwith contributions from Francesco di Filippo and Enzo Cocca",
    "crumbs": [
      "Project Info",
      "Version History"
    ]
  },
  {
    "objectID": "version_history.html#version-history",
    "href": "version_history.html#version-history",
    "title": "Version History",
    "section": "Version History",
    "text": "Version History\n\nVersion 0.0.3 (July 2025)\nUpdate to the PyPotteryInk package, with a few improvements\nKey Features:\n\nüìå Added new model 6h-MC model as default üìå\nAdded a test_imgto visualize the performance of each model\nReworking of the test.py module: now it test the library on the test_img\nAdded downscaling options in process_single_image and process_folder\n\nModels:\n\nFine-tuned example for Monte Cimino assemblage (6h-MC)\nFine-tuned model for historical and painted pottery (4h-PAINT)",
    "crumbs": [
      "Project Info",
      "Version History"
    ]
  },
  {
    "objectID": "version_history.html#version-history-1",
    "href": "version_history.html#version-history-1",
    "title": "Version History",
    "section": "Version History",
    "text": "Version History\n\nVersion 0.0.2 (April 2025)\nA little update to the PyPotteryInk package, with a few bug fixes and improvements.\nKey Features:\n\nAdded an AI disclosure reminder when running the process_folder function\nAdded a Google Colab notebook for easy testing of the package\nFixed some errors in the documentation\n\n\n\nVersion 0.0.1 (January 2025)\nInitial public release of PyPotteryInk.\nKey Features:\n\nImage-to-image translation using pix2pix-turbo architecture\nAdaptive patch-based processing for high-resolution drawings\nComprehensive preprocessing and postprocessing tools\nWeb documentation\n\nModels:\n\nBase model trained on 500 protohistoric pottery drawings (10k)\nFine-tuned example for Monte Croce Guardia assemblage (6h-MCG)",
    "crumbs": [
      "Project Info",
      "Version History"
    ]
  },
  {
    "objectID": "models_zoo.html",
    "href": "models_zoo.html",
    "title": "PyPotteryInk Model Zoo",
    "section": "",
    "text": "This page lists the available pre-trained models for PyPotteryInk. Each model is show specific pros and cons. You can use these models directly or fine-tune them for your specific needs.",
    "crumbs": [
      "Project Info",
      "Model zoo"
    ]
  },
  {
    "objectID": "models_zoo.html#overview",
    "href": "models_zoo.html#overview",
    "title": "PyPotteryInk Model Zoo",
    "section": "",
    "text": "This page lists the available pre-trained models for PyPotteryInk. Each model is show specific pros and cons. You can use these models directly or fine-tune them for your specific needs.",
    "crumbs": [
      "Project Info",
      "Model zoo"
    ]
  },
  {
    "objectID": "models_zoo.html#benchmark-image",
    "href": "models_zoo.html#benchmark-image",
    "title": "PyPotteryInk Model Zoo",
    "section": "Benchmark image",
    "text": "Benchmark image\nThe benchmark image contains a variety of images and styles for testing each model. It can be used to quickly assess the quality of the output. As the library will be used for a variety of different styles and morphologies, new images will be added to the benchmark images.",
    "crumbs": [
      "Project Info",
      "Model zoo"
    ]
  },
  {
    "objectID": "models_zoo.html#available-models",
    "href": "models_zoo.html#available-models",
    "title": "PyPotteryInk Model Zoo",
    "section": "Available Models",
    "text": "Available Models\n\n10k\nDescription: The base model trained on a diverse dataset of 492 archaeological drawings from multiple contexts including Casinalbo, Montale, Monte Croce Guardia, and Monte Cimino sites.\n\nTraining Data: 492 paired drawings\nResolution: 512√ó512 pixels\nTraining Steps: 10,000\nBest For: General purpose archaeological drawing inking, starting point for fine-tuning\n\nParameters used for training:\n\n\n\nParameter\n10k\n\n\n\n\nadam_beta1\n0.9\n\n\nadam_beta2\n0.999\n\n\nadam_epsilon\n0.00000001\n\n\nadam_weight_decay\n0.01\n\n\nallow_tf32\nfalse\n\n\ncheckpointing_steps\n500\n\n\ndataloader_num_workers\n0\n\n\nenable_xformers_memory_efficient_attention\ntrue\n\n\neval_freq\n100\n\n\ngan_disc_type\n‚Äúvagan_clip‚Äù\n\n\ngan_loss_type\n‚Äúmultilevel_sigmoid_s‚Äù\n\n\ngradient_accumulation_steps\n1\n\n\ngradient_checkpointing\nfalse\n\n\nlambda_clipsim\n5\n\n\nlambda_gan\n0.5\n\n\nlambda_l2\n1\n\n\nlambda_lpips\n5\n\n\nlearning_rate\n0.000005\n\n\nlora_rank_unet\n8\n\n\nlora_rank_vae\n4\n\n\nlr_num_cycles\n1\n\n\nlr_power\n1\n\n\nlr_scheduler\n‚Äúconstant‚Äù\n\n\nlr_warmup_steps\n500\n\n\nmax_grad_norm\n1\n\n\nmax_train_steps\n10,000\n\n\nmixed_precision\n‚Äúno‚Äù\n\n\nnum_samples_eval\n100\n\n\nnum_training_epochs\n1\n\n\npretrained_model_name_or_path\n‚Äústabilityai/sd-turbo‚Äù\n\n\nresolution\n512\n\n\nset_grads_to_none\nfalse\n\n\ntest_image_prep\n‚Äúresized_crop_512‚Äù\n\n\ntrack_val_fid\ntrue\n\n\ntrain_batch_size\n2\n\n\ntrain_image_prep\n‚Äúresized_crop_512‚Äù\n\n\nviz_freq\n25\n\n\n\nAuthor: Lorenzo Cardarelli\n\n\nBenchmark\n\n\n\n\n\n\n\n\n6h-MCG\nDescription: Fine-tuned model specialized for Monte Croce Guardia style pottery drawings. Optimized for high-detail preservation and consistent stippling patterns.\n\nTraining Data: 9 paired drawings from Monte Croce Guardia\nBase Model: 10k Base Model\nResolution: 512√ó512 pixels\nTraining Steps: 600\nBest For: Middle Bronze Age / Recent Bronze Age pottery\n\nParameters used for training:\n\n\n\nParameter\n6h-MCG\n\n\n\n\nadam_beta1\n0.9\n\n\nadam_beta2\n0.999\n\n\nadam_epsilon\n0.00000001\n\n\nadam_weight_decay\n0.01\n\n\nallow_tf32\nfalse\n\n\ncheckpointing_steps\n100\n\n\ndataloader_num_workers\n4\n\n\nenable_xformers_memory_efficient_attention\ntrue\n\n\neval_freq\n100\n\n\ngan_disc_type\n‚Äúvagan_clip‚Äù\n\n\ngan_loss_type\n‚Äúmultilevel_sigmoid_s‚Äù\n\n\ngradient_accumulation_steps\n4\n\n\ngradient_checkpointing\nfalse\n\n\nlambda_clipsim\n3\n\n\nlambda_gan\n0.9\n\n\nlambda_l2\n1\n\n\nlambda_lpips\n10\n\n\nlearning_rate\n0.00001\n\n\nlora_rank_unet\n128\n\n\nlora_rank_vae\n48\n\n\nlr_num_cycles\n1\n\n\nlr_power\n1\n\n\nlr_scheduler\n‚Äúconstant‚Äù\n\n\nlr_warmup_steps\n200\n\n\nmax_grad_norm\n1\n\n\nmax_train_steps\n600\n\n\nmixed_precision\n‚Äúno‚Äù\n\n\nnum_samples_eval\n100\n\n\nnum_training_epochs\n1\n\n\npretrained_model_name_or_path\n./10k.pkl\n\n\nresolution\n512\n\n\nset_grads_to_none\nfalse\n\n\ntest_image_prep\n‚Äúresized_crop_512‚Äù\n\n\ntrack_val_fid\ntrue\n\n\ntrain_batch_size\n1\n\n\ntrain_image_prep\n‚Äúresized_crop_512‚Äù\n\n\nviz_freq\n50\n\n\n\nAuthor: Lorenzo Cardarelli\n\n\nBenchmark\n\n\n\n\n\n\n\n\n6h-MC\nDescription: Fine-tuned model specialized for Monte Cimino style pottery drawings. Optimized for high-detail preservation and engraved decoration.\n\nTraining Data: 15 paired drawings from Monte Cimino\nBase Model: 10k Base Model\nResolution: 512√ó512 pixels\nTraining Steps: 600\nBest For: Recent Bronze Age pottery / Final Bronze Age / Historic Age\n\nParameters used for training:\n\n\n\nParameter\n6h-MC\n\n\n\n\nadam_beta1\n0.9\n\n\nadam_beta2\n0.999\n\n\nadam_epsilon\n0.00000001\n\n\nadam_weight_decay\n0.01\n\n\nallow_tf32\nfalse\n\n\ncheckpointing_steps\n100\n\n\ndataloader_num_workers\n4\n\n\nenable_xformers_memory_efficient_attention\ntrue\n\n\neval_freq\n100\n\n\ngan_disc_type\n‚Äúvagan_clip‚Äù\n\n\ngan_loss_type\n‚Äúmultilevel_sigmoid_s‚Äù\n\n\ngradient_accumulation_steps\n4\n\n\ngradient_checkpointing\nfalse\n\n\nlambda_clipsim\n3\n\n\nlambda_gan\n0.9\n\n\nlambda_l2\n1\n\n\nlambda_lpips\n10\n\n\nlearning_rate\n0.00001\n\n\nlora_rank_unet\n256\n\n\nlora_rank_vae\n48\n\n\nlr_num_cycles\n1\n\n\nlr_power\n1\n\n\nlr_scheduler\n‚Äúcosine‚Äù\n\n\nlr_warmup_steps\n200\n\n\nmax_grad_norm\n1\n\n\nmax_train_steps\n600\n\n\nmixed_precision\n‚Äúno‚Äù\n\n\nnum_samples_eval\n100\n\n\nnum_training_epochs\n1\n\n\npretrained_model_name_or_path\n./10k.pkl\n\n\nresolution\n512\n\n\nset_grads_to_none\nfalse\n\n\ntest_image_prep\n‚Äúresized_crop_512‚Äù\n\n\ntrack_val_fid\ntrue\n\n\ntrain_batch_size\n1\n\n\ntrain_image_prep\n‚Äúresized_crop_512‚Äù\n\n\nviz_freq\n50\n\n\n\nAuthor: Lorenzo Cardarelli\n\n\nBenchmark\n\n\n\n\n\n\n\n\n4h-PAINT\nDescription: Fine-tuned model specialized for painted decoration and historical pottery.\n\nTraining Data: 15 paired drawings from Late Bronze Age / Early Iron Age of Southern Italy\nBase Model: 6h-MC\nResolution: 512√ó512 pixels\nTraining Steps: 400\nBest For: Historic Age / Painted Pottery (No shadows!)\n\nParameters used for training:\n\n\n\nParameter\n4h-PAINT\n\n\n\n\nadam_beta1\n0.9\n\n\nadam_beta2\n0.999\n\n\nadam_epsilon\n0.00000001\n\n\nadam_weight_decay\n0.01\n\n\nallow_tf32\nfalse\n\n\ncheckpointing_steps\n100\n\n\ndataloader_num_workers\n4\n\n\nenable_xformers_memory_efficient_attention\ntrue\n\n\neval_freq\n100\n\n\ngan_disc_type\n‚Äúvagan_clip‚Äù\n\n\ngan_loss_type\n‚Äúmultilevel_sigmoid_s‚Äù\n\n\ngradient_accumulation_steps\n4\n\n\ngradient_checkpointing\nfalse\n\n\nlambda_clipsim\n3\n\n\nlambda_gan\n0.9\n\n\nlambda_l2\n1\n\n\nlambda_lpips\n10\n\n\nlearning_rate\n0.00001\n\n\nlora_rank_unet\n256\n\n\nlora_rank_vae\n48\n\n\nlr_num_cycles\n1\n\n\nlr_power\n1\n\n\nlr_scheduler\n‚Äúcosine‚Äù\n\n\nlr_warmup_steps\n200\n\n\nmax_grad_norm\n1\n\n\nmax_train_steps\n400\n\n\nmixed_precision\n‚Äúno‚Äù\n\n\nnum_samples_eval\n100\n\n\nnum_training_epochs\n1\n\n\npretrained_model_name_or_path\n./6h-MC.pkl\n\n\nresolution\n512\n\n\nset_grads_to_none\nfalse\n\n\ntest_image_prep\n‚Äúresized_crop_512‚Äù\n\n\ntrack_val_fid\ntrue\n\n\ntrain_batch_size\n1\n\n\ntrain_image_prep\n‚Äúresized_crop_512‚Äù\n\n\nviz_freq\n50\n\n\n\nAuthor: Lorenzo Cardarelli, Elisa Pizzuti\n\n\nBenchmark",
    "crumbs": [
      "Project Info",
      "Model zoo"
    ]
  },
  {
    "objectID": "docs_user_interface.html",
    "href": "docs_user_interface.html",
    "title": "PyPotteryLens: The user interface",
    "section": "",
    "text": "The GUI\n\n\n\n\n\n\nNote\n\n\n\nThis page is under construction.",
    "crumbs": [
      "Documentation",
      "User Interface"
    ]
  },
  {
    "objectID": "docs_postprocessing.html",
    "href": "docs_postprocessing.html",
    "title": "PyPotteryLens: Post-Processing Tools for Archaeological Drawings",
    "section": "",
    "text": "Introduction\nAfter generating inked versions of archaeological drawings, we often need to refine the results to meet some publication requirements or enhance the result. The post-processing module provides tools for image binarization, background removal, and stippling pattern enhancement. This guide will walk you through the main functions and best practices for using these tools.\n\n\nRequired Libraries\nLet‚Äôs start by importing the necessary functions:\nfrom postprocessing import (\n    binarize_image,\n    remove_white_background,\n    process_image_binarize,\n    binarize_folder_images,\n    enhance_stippling,\n    modify_stippling,\n    control_stippling\n)\n\n\nBinarization and Background Removal\nFor many publication contexts, we want to remove the white background to create binary transparent images that can be easily integrated into figures:\n\n# You can combine binarization and background removal in one step:\nprocessed_image = process_image_binarize(\n    image_path=\"path/to/image.jpg\",\n    binarize_threshold=127,\n    white_threshold=250,\n    save_path=\"output.png\"  # Must use PNG to preserve transparency\n)\nHere the result:\n\n\n\nOriginal ink\n\n\n\n\n\nBinarized and background-clear image\n\n\n\n\nBatch Processing\nWhen working with multiple drawings, you can process an entire folder at once:\nbinarize_folder_images(\n    input_folder=\"path/to/drawings\",\n    binarize_threshold=127,\n    white_threshold=250\n)\nThis creates a new folder with ‚Äú_binarized‚Äù suffix containing processed images\n\n\nAdvanced Stippling Control\nBy using the post-processing module you can enhance and modify stippling patterns, check out this example!\n\nEnhancing Stippling Patterns\nFirst, we can isolate and enhance existing stippling patterns:\n# Separate the main drawing from stippling patterns\nprocessed_img, stippling_pattern = enhance_stippling(\n    img=Image.open(\"drawing.png\"),\n    min_size=80,     # Minimum size of dots to preserve\n    connectivity=2    # How dots connect to form patterns\n)\nThe function returns two images:\n\nprocessed_img: Main drawing without small dots\nstippling_pattern: Isolated stippling pattern\n\n\n\n\nImage without dotting\n\n\n\n\n\nIsolated dotting\n\n\n\n\nModifying Stippling\nOnce we‚Äôve isolated the stippling patterns, we can modify them in several ways:\nmodified_image = modify_stippling(\n    processed_img=processed_img,\n    stippling_pattern=stippling_pattern,\n    operation='dilate',  # Options: 'dilate', 'fade', or 'both'\n    intensity=0.5,      # Controls strength of dilation (0.0-1.0)\n    opacity=1.0         # Controls darkness of stippling (0.0-1.0)\n)\nThe operation parameter offers three different modification approaches:\n\n‚Äòdilate‚Äô: Makes dots larger while maintaining their density\n‚Äòfade‚Äô: Adjusts the opacity of dots without changing their size\n‚Äòboth‚Äô: Combines dilation and opacity adjustments\n\nHere a bunch of examples:\nFirst, we increase the dots using the ‚Äúdilate‚Äù option:\n Secondly, we try to adjust the opacity using the fade option:\n\n\n\n\n\nFinally, we combine the techniques:\n\n\n\n\n\n\n\nBatch Stippling Control\nFor consistent stippling across multiple drawings:\ncontrol_stippling(\n    input_folder=\"path/to/drawings\",\n    min_size=50,          # Size threshold for dot detection\n    connectivity=2,        # How dots connect to each other\n    operation='fade',      # Type of modification\n    intensity=0.5,        # Strength of effect\n    opacity=0.5           # Final opacity of stippling\n)\nThis creates a new folder with ‚Äú_dotting_modified‚Äù suffix containing the processed images.\n\n\n\nUnderstanding the Parameters\nWhen working with the stippling controls, it‚Äôs important to understand how different parameters affect the result:\n\nmin_size: Controls which dots are considered part of stippling patterns versus actual drawing elements. Larger values preserve only larger dots.\nconnectivity: Determines how dots are grouped together. Higher values (2 or 3) allow more diagonal connections.\nintensity: When dilating, controls how much dots expand. Values between 0.3-0.7 usually give the best results.\nopacity: Controls the final darkness of stippling patterns. Lower values create lighter shading.\n\n\n\nBest Practices\n\nAlways work with high-resolution images to ensure clean binarization.\nStart with conservative threshold values and adjust as needed.\nSave intermediate results when working with stippling modifications.\nTest your parameters on a small sample before processing entire collections.",
    "crumbs": [
      "Documentation",
      "Postprocessing"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "For general questions, bug reports, or feature requests, you can:\n\nCheck the GitHub repository for existing issues\nSubmit new issues through GitHub‚Äôs issue tracker\nReview the documentation for common solutions",
    "crumbs": [
      "Project Info",
      "Contact"
    ]
  },
  {
    "objectID": "contact.html#general-support",
    "href": "contact.html#general-support",
    "title": "Contact",
    "section": "",
    "text": "For general questions, bug reports, or feature requests, you can:\n\nCheck the GitHub repository for existing issues\nSubmit new issues through GitHub‚Äôs issue tracker\nReview the documentation for common solutions",
    "crumbs": [
      "Project Info",
      "Contact"
    ]
  },
  {
    "objectID": "contact.html#research-collaboration-and-custom-training-services",
    "href": "contact.html#research-collaboration-and-custom-training-services",
    "title": "Contact",
    "section": "Research Collaboration and Custom Training Services",
    "text": "Research Collaboration and Custom Training Services\nI am interested in collaborating with archaeological projects and institutions to:\n\nDevelop new features for specific archaeological needs\nExpand capabilities to new pottery styles\nImprove accuracy for various documentation styles\n\nAs archaeological collections vary significantly in style and complexity:\n\nCustom model training for your pottery styles\nFine-tuning for your archaeological contexts",
    "crumbs": [
      "Project Info",
      "Contact"
    ]
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "Get in Touch",
    "text": "Get in Touch\n\n\n\n\n\n\nLet‚Äôs enhance your archaeological documentation\n\n\n\nI‚Äôm happy to discuss how PyPotteryInk can be optimized for your needs:\nüìß Email: lorenzo.cardarelli@uniroma1.it",
    "crumbs": [
      "Project Info",
      "Contact"
    ]
  },
  {
    "objectID": "api_postprocessing.html",
    "href": "api_postprocessing.html",
    "title": "Postprocessing Module",
    "section": "",
    "text": "The postprocessing module provides tools for refining and enhancing the converted archaeological drawings, focusing on output quality and archaeological detail preservation.\n\n\ndef binarize_image(\n    image: Union[PIL.Image, np.ndarray],\n    threshold: int = 127\n) -&gt; PIL.Image\nConverts grayscale drawings to binary format, ideal for final publication preparation.\n\n\n\nimage\n\nInput image (PIL Image or numpy array)\n\nthreshold\n\nIntensity threshold (0-255)\n\n\n\n\n\nbinary = binarize_image(\"processed_vessel.png\", threshold=150)\n\n\n\n\ndef remove_white_background(\n    image: PIL.Image,\n    threshold: int = 250\n) -&gt; PIL.Image\nCreates transparent backgrounds for archaeological drawings, useful for figure composition.\n\n\n\nimage\n\nInput drawing\n\nthreshold\n\nValue above which pixels are considered white\n\n\n\n\n\ntransparent = remove_white_background(\"vessel.png\", threshold=245)\n\n\n\n\ndef process_image_binarize(\n    image_path: str,\n    binarize_threshold: int = 127,\n    white_threshold: int = 250,\n    save_path: Optional[str] = None\n) -&gt; PIL.Image\nCombined function for binarization and background removal.\n\n\n\nimage_path\n\nPath to input image\n\nbinarize_threshold\n\nThreshold for black/white conversion\n\nwhite_threshold\n\nThreshold for transparency\n\nsave_path\n\nOptional output path\n\n\n\n\n\n\ndef binarize_folder_images(\n    input_folder: str,\n    binarize_threshold: int = 127,\n    white_threshold: int = 250\n) -&gt; None\nBatch processes a folder of drawings, applying binarization and background removal.\n\n\n\ninput_folder\n\nDirectory containing drawings\n\nbinarize_threshold\n\nThreshold for binarization\n\nwhite_threshold\n\nThreshold for transparency\n\n\n\n\n\n\ndef enhance_stippling(\n    img: PIL.Image,\n    min_size: int = 80,\n    connectivity: int = 2\n) -&gt; Tuple[PIL.Image, PIL.Image]\nIsolates and enhances stippling patterns in archaeological drawings.\n\n\n\nimg\n\nInput drawing\n\nmin_size\n\nMinimum object size to preserve\n\nconnectivity\n\nConnection parameter for pattern detection\n\n\n\n\n\nReturns tuple containing: - processed_image: Drawing with enhanced stippling - stippling_pattern: Isolated stippling mask\n\n\n\n\ndef modify_stippling(\n    processed_img: PIL.Image,\n    stippling_pattern: PIL.Image,\n    operation: str = 'dilate',\n    intensity: float = 0.5,\n    opacity: float = 1.0\n) -&gt; PIL.Image\nAdjusts stippling patterns through morphological operations and intensity modulation.\n\n\n\nprocessed_img\n\nBase image without stippling\n\nstippling_pattern\n\nIsolated stippling pattern\n\noperation\n\nType of modification (‚Äòdilate‚Äô, ‚Äòfade‚Äô, or ‚Äòboth‚Äô)\n\nintensity\n\nMorphological modification strength (0.0-1.0)\n\nopacity\n\nStippling opacity factor (0.0-1.0)\n\n\n\n\n\nenhanced = modify_stippling(\n    base_img,\n    dots_pattern,\n    operation='both',\n    intensity=0.7,\n    opacity=0.8\n)\n\n\n\n\ndef control_stippling(\n    input_folder: str,\n    min_size: int = 50,\n    connectivity: int = 2,\n    operation: str = 'fade',\n    intensity: float = 0.5,\n    opacity: float = 0.5\n) -&gt; None\nBatch processes stippling patterns in a folder of archaeological drawings.\n\n\n\ninput_folder\n\nDirectory containing drawings\n\nmin_size\n\nMinimum object size to preserve\n\nconnectivity\n\nPattern detection parameter\n\noperation\n\nModification type (‚Äòdilate‚Äô, ‚Äòfade‚Äô, ‚Äòboth‚Äô)\n\nintensity\n\nModification strength\n\nopacity\n\nPattern opacity\n\n\n\n\n\ncontrol_stippling(\n    \"vessel_drawings/\",\n    min_size=60,\n    operation='both',\n    intensity=0.6\n)",
    "crumbs": [
      "API Documentation",
      "postprocessing"
    ]
  },
  {
    "objectID": "api_postprocessing.html#binarize-image",
    "href": "api_postprocessing.html#binarize-image",
    "title": "Postprocessing Module",
    "section": "",
    "text": "def binarize_image(\n    image: Union[PIL.Image, np.ndarray],\n    threshold: int = 127\n) -&gt; PIL.Image\nConverts grayscale drawings to binary format, ideal for final publication preparation.\n\n\n\nimage\n\nInput image (PIL Image or numpy array)\n\nthreshold\n\nIntensity threshold (0-255)\n\n\n\n\n\nbinary = binarize_image(\"processed_vessel.png\", threshold=150)",
    "crumbs": [
      "API Documentation",
      "postprocessing"
    ]
  },
  {
    "objectID": "api_postprocessing.html#remove-white-background",
    "href": "api_postprocessing.html#remove-white-background",
    "title": "Postprocessing Module",
    "section": "",
    "text": "def remove_white_background(\n    image: PIL.Image,\n    threshold: int = 250\n) -&gt; PIL.Image\nCreates transparent backgrounds for archaeological drawings, useful for figure composition.\n\n\n\nimage\n\nInput drawing\n\nthreshold\n\nValue above which pixels are considered white\n\n\n\n\n\ntransparent = remove_white_background(\"vessel.png\", threshold=245)",
    "crumbs": [
      "API Documentation",
      "postprocessing"
    ]
  },
  {
    "objectID": "api_postprocessing.html#process-image-binarize",
    "href": "api_postprocessing.html#process-image-binarize",
    "title": "Postprocessing Module",
    "section": "",
    "text": "def process_image_binarize(\n    image_path: str,\n    binarize_threshold: int = 127,\n    white_threshold: int = 250,\n    save_path: Optional[str] = None\n) -&gt; PIL.Image\nCombined function for binarization and background removal.\n\n\n\nimage_path\n\nPath to input image\n\nbinarize_threshold\n\nThreshold for black/white conversion\n\nwhite_threshold\n\nThreshold for transparency\n\nsave_path\n\nOptional output path",
    "crumbs": [
      "API Documentation",
      "postprocessing"
    ]
  },
  {
    "objectID": "api_postprocessing.html#binarize-folder-images",
    "href": "api_postprocessing.html#binarize-folder-images",
    "title": "Postprocessing Module",
    "section": "",
    "text": "def binarize_folder_images(\n    input_folder: str,\n    binarize_threshold: int = 127,\n    white_threshold: int = 250\n) -&gt; None\nBatch processes a folder of drawings, applying binarization and background removal.\n\n\n\ninput_folder\n\nDirectory containing drawings\n\nbinarize_threshold\n\nThreshold for binarization\n\nwhite_threshold\n\nThreshold for transparency",
    "crumbs": [
      "API Documentation",
      "postprocessing"
    ]
  },
  {
    "objectID": "api_postprocessing.html#enhance-stippling",
    "href": "api_postprocessing.html#enhance-stippling",
    "title": "Postprocessing Module",
    "section": "",
    "text": "def enhance_stippling(\n    img: PIL.Image,\n    min_size: int = 80,\n    connectivity: int = 2\n) -&gt; Tuple[PIL.Image, PIL.Image]\nIsolates and enhances stippling patterns in archaeological drawings.\n\n\n\nimg\n\nInput drawing\n\nmin_size\n\nMinimum object size to preserve\n\nconnectivity\n\nConnection parameter for pattern detection\n\n\n\n\n\nReturns tuple containing: - processed_image: Drawing with enhanced stippling - stippling_pattern: Isolated stippling mask",
    "crumbs": [
      "API Documentation",
      "postprocessing"
    ]
  },
  {
    "objectID": "api_postprocessing.html#modify-stippling",
    "href": "api_postprocessing.html#modify-stippling",
    "title": "Postprocessing Module",
    "section": "",
    "text": "def modify_stippling(\n    processed_img: PIL.Image,\n    stippling_pattern: PIL.Image,\n    operation: str = 'dilate',\n    intensity: float = 0.5,\n    opacity: float = 1.0\n) -&gt; PIL.Image\nAdjusts stippling patterns through morphological operations and intensity modulation.\n\n\n\nprocessed_img\n\nBase image without stippling\n\nstippling_pattern\n\nIsolated stippling pattern\n\noperation\n\nType of modification (‚Äòdilate‚Äô, ‚Äòfade‚Äô, or ‚Äòboth‚Äô)\n\nintensity\n\nMorphological modification strength (0.0-1.0)\n\nopacity\n\nStippling opacity factor (0.0-1.0)\n\n\n\n\n\nenhanced = modify_stippling(\n    base_img,\n    dots_pattern,\n    operation='both',\n    intensity=0.7,\n    opacity=0.8\n)",
    "crumbs": [
      "API Documentation",
      "postprocessing"
    ]
  },
  {
    "objectID": "api_postprocessing.html#control-stippling",
    "href": "api_postprocessing.html#control-stippling",
    "title": "Postprocessing Module",
    "section": "",
    "text": "def control_stippling(\n    input_folder: str,\n    min_size: int = 50,\n    connectivity: int = 2,\n    operation: str = 'fade',\n    intensity: float = 0.5,\n    opacity: float = 0.5\n) -&gt; None\nBatch processes stippling patterns in a folder of archaeological drawings.\n\n\n\ninput_folder\n\nDirectory containing drawings\n\nmin_size\n\nMinimum object size to preserve\n\nconnectivity\n\nPattern detection parameter\n\noperation\n\nModification type (‚Äòdilate‚Äô, ‚Äòfade‚Äô, ‚Äòboth‚Äô)\n\nintensity\n\nModification strength\n\nopacity\n\nPattern opacity\n\n\n\n\n\ncontrol_stippling(\n    \"vessel_drawings/\",\n    min_size=60,\n    operation='both',\n    intensity=0.6\n)",
    "crumbs": [
      "API Documentation",
      "postprocessing"
    ]
  },
  {
    "objectID": "api_ink.html",
    "href": "api_ink.html",
    "title": "Ink Module",
    "section": "",
    "text": "The ink module provides the foundation for transforming archaeological pencil drawings into publication-ready inked versions. Each function serves a specific purpose in the workflow, carefully preserving archaeological details while ensuring professional output quality.\n\n\ndef process_single_image(\n    input_image_path_or_pil: Union[str, Image.Image],\n    prompt: str,\n    model_path: str,\n    output_dir: str = 'output',\n    use_fp16: bool = False,\n    output_name: Optional[str] = None,\n    contrast_scale: float = 1,\n    return_pil: bool = False,\n    patch_size: int = 512,\n    overlap: int = 64,\n    upscale: float = 1,\n) -&gt; Union[str, Image.Image]\nThis function handles the conversion of individual drawings, providing fine control over the processing parameters. Think of it as a digital artisan, carefully converting each drawing while maintaining archaeological accuracy.\n\n\n\ninput_image_path_or_pil\n\nYour drawing to process - either a file path or a PIL Image\n\nprompt\n\nInstructions for the model, typically ‚Äúmake it ready for publication‚Äù\n\nmodel_path\n\nLocation of your trained model file\n\n\n\n\n\n\ncontrast_scale ¬∑ default: 1.0\n\nFine-tunes the intensity of lines and shading. Values between 1.25-1.5 often work best for archaeological materials.\n\npatch_size ¬∑ default: 512\n\nProcessing segment size. Like dividing a large drawing into manageable sections.\n\noverlap ¬∑ default: 64\n\nControls smooth transitions between processed sections.\n\nupscale ¬∑ default: 1\n\nUpscaling or downscaling factor for processing. It doesn‚Äôt affect the output size\n\n\n\n\n\nBasic processing:\nresult = process_single_image(\n    \"vessel_123.jpg\",\n    prompt=\"make it ready for publication\",\n    model_path=\"model_601.pkl\"\n)\n\n\n\n\ndef process_folder(\n    input_folder: str,\n    model_path: str,\n    prompt: str = \"make it ready for publication\",\n    output_dir: str = 'output',\n    use_fp16: bool = False,\n    contrast_scale: float = 1,\n    patch_size: int = 512,\n    overlap: int = 64,\n    file_extensions: tuple = ('.jpg', '.jpeg', '.png'),\n    upscale: float = 1,\n) -&gt; dict\nBatch processes a directory of archaeological drawings, maintaining consistency across the entire collection. The function tracks progress and generates detailed logs and comparisons.\n\n\n\ninput_folder\n\nDirectory containing your archaeological drawings\n\nmodel_path\n\nLocation of your trained model file\n\n\n\n\n\n\ncontrast_scale ¬∑ default: 1.0\n\nGlobal contrast adjustment for the entire batch\n\nfile_extensions ¬∑ default: (‚Äò.jpg‚Äô, ‚Äò.jpeg‚Äô, ‚Äò.png‚Äô)\n\nSupported file types to process\n\nupscale ¬∑ default: 1\n\nUpscaling or downscaling factor for processing. It doesn‚Äôt affect the output size\n\n\n\n\n\nReturns a dictionary containing: - successful: Number of successfully processed images - failed: Number of failed conversions - failed_files: List of problematic files - average_time: Mean processing time per image - log_file: Path to detailed processing log - comparison_dir: Path to before/after comparisons\n\n\n\nProcess all drawings in a directory:\nresults = process_folder(\n    \"excavation_2024_drawings/\",\n    model_path=\"model_601.pkl\",\n    contrast_scale=1.25\n)\n\n\n\n\ndef run_diagnostics(\n    input_folder: str,\n    model_path: str,\n    prompt: str = \"make it ready for publication\",\n    patch_size: int = 512,\n    overlap: int = 64,\n    num_sample_images: int = 5,\n    contrast_values: list = [0.5, 0.75, 1, 1.5, 2, 3],\n    output_dir: str = 'diagnostics'\n) -&gt; None\nPerforms preliminary analysis on your dataset to optimize processing parameters. Creates visualizations of patch divisions and contrast effects to help fine-tune settings.\n\n\n\ninput_folder\n\nDirectory with sample drawings to analyse\n\nmodel_path\n\nLocation of your trained model file\n\n\n\n\n\n\nnum_sample_images ¬∑ default: 5\n\nNumber of drawings to analyse (max 5)\n\ncontrast_values ¬∑ default: [0.5, 0.75, 1, 1.5, 2, 3]\n\nContrast levels to test\n\n\n\n\n\n\nPatch visualization diagrams\nContrast effect comparisons\nImage summary statistics\nProcessing recommendations\n\n\n\n\nRun analysis on a new dataset:\nrun_diagnostics(\n    \"new_site_drawings/\",\n    model_path=\"model_601.pkl\",\n    contrast_values=[0.75, 1, 1.25, 1.5]\n)\n\n\n\n\ndef calculate_patches(\n    width: int,\n    height: int,\n    patch_size: int = 512,\n    overlap: int = 64\n) -&gt; tuple[int, int, int]\nInternal utility that determines optimal patch division for processing large drawings. Ensures efficient memory usage while maintaining detail preservation.\n\n\n\nwidth, height\n\nImage dimensions in pixels\n\npatch_size ¬∑ default: 512\n\nSize of processing segments\n\noverlap ¬∑ default: 64\n\nOverlap between segments\n\n\n\n\n\nReturns a tuple containing:\n\ntotal_patches: Total number of segments\npatches_per_row: Number of patches horizontally\nnum_rows: Number of patches vertically\n\n\n\n\nCalculate processing segments:\npatches, rows, cols = calculate_patches(2048, 1536)\nprint(f\"Processing in {patches} segments\")",
    "crumbs": [
      "API Documentation",
      "ink"
    ]
  },
  {
    "objectID": "api_ink.html#process-single-image",
    "href": "api_ink.html#process-single-image",
    "title": "Ink Module",
    "section": "",
    "text": "def process_single_image(\n    input_image_path_or_pil: Union[str, Image.Image],\n    prompt: str,\n    model_path: str,\n    output_dir: str = 'output',\n    use_fp16: bool = False,\n    output_name: Optional[str] = None,\n    contrast_scale: float = 1,\n    return_pil: bool = False,\n    patch_size: int = 512,\n    overlap: int = 64,\n    upscale: float = 1,\n) -&gt; Union[str, Image.Image]\nThis function handles the conversion of individual drawings, providing fine control over the processing parameters. Think of it as a digital artisan, carefully converting each drawing while maintaining archaeological accuracy.\n\n\n\ninput_image_path_or_pil\n\nYour drawing to process - either a file path or a PIL Image\n\nprompt\n\nInstructions for the model, typically ‚Äúmake it ready for publication‚Äù\n\nmodel_path\n\nLocation of your trained model file\n\n\n\n\n\n\ncontrast_scale ¬∑ default: 1.0\n\nFine-tunes the intensity of lines and shading. Values between 1.25-1.5 often work best for archaeological materials.\n\npatch_size ¬∑ default: 512\n\nProcessing segment size. Like dividing a large drawing into manageable sections.\n\noverlap ¬∑ default: 64\n\nControls smooth transitions between processed sections.\n\nupscale ¬∑ default: 1\n\nUpscaling or downscaling factor for processing. It doesn‚Äôt affect the output size\n\n\n\n\n\nBasic processing:\nresult = process_single_image(\n    \"vessel_123.jpg\",\n    prompt=\"make it ready for publication\",\n    model_path=\"model_601.pkl\"\n)",
    "crumbs": [
      "API Documentation",
      "ink"
    ]
  },
  {
    "objectID": "api_ink.html#process-folder",
    "href": "api_ink.html#process-folder",
    "title": "Ink Module",
    "section": "",
    "text": "def process_folder(\n    input_folder: str,\n    model_path: str,\n    prompt: str = \"make it ready for publication\",\n    output_dir: str = 'output',\n    use_fp16: bool = False,\n    contrast_scale: float = 1,\n    patch_size: int = 512,\n    overlap: int = 64,\n    file_extensions: tuple = ('.jpg', '.jpeg', '.png'),\n    upscale: float = 1,\n) -&gt; dict\nBatch processes a directory of archaeological drawings, maintaining consistency across the entire collection. The function tracks progress and generates detailed logs and comparisons.\n\n\n\ninput_folder\n\nDirectory containing your archaeological drawings\n\nmodel_path\n\nLocation of your trained model file\n\n\n\n\n\n\ncontrast_scale ¬∑ default: 1.0\n\nGlobal contrast adjustment for the entire batch\n\nfile_extensions ¬∑ default: (‚Äò.jpg‚Äô, ‚Äò.jpeg‚Äô, ‚Äò.png‚Äô)\n\nSupported file types to process\n\nupscale ¬∑ default: 1\n\nUpscaling or downscaling factor for processing. It doesn‚Äôt affect the output size\n\n\n\n\n\nReturns a dictionary containing: - successful: Number of successfully processed images - failed: Number of failed conversions - failed_files: List of problematic files - average_time: Mean processing time per image - log_file: Path to detailed processing log - comparison_dir: Path to before/after comparisons\n\n\n\nProcess all drawings in a directory:\nresults = process_folder(\n    \"excavation_2024_drawings/\",\n    model_path=\"model_601.pkl\",\n    contrast_scale=1.25\n)",
    "crumbs": [
      "API Documentation",
      "ink"
    ]
  },
  {
    "objectID": "api_ink.html#run-diagnostics",
    "href": "api_ink.html#run-diagnostics",
    "title": "Ink Module",
    "section": "",
    "text": "def run_diagnostics(\n    input_folder: str,\n    model_path: str,\n    prompt: str = \"make it ready for publication\",\n    patch_size: int = 512,\n    overlap: int = 64,\n    num_sample_images: int = 5,\n    contrast_values: list = [0.5, 0.75, 1, 1.5, 2, 3],\n    output_dir: str = 'diagnostics'\n) -&gt; None\nPerforms preliminary analysis on your dataset to optimize processing parameters. Creates visualizations of patch divisions and contrast effects to help fine-tune settings.\n\n\n\ninput_folder\n\nDirectory with sample drawings to analyse\n\nmodel_path\n\nLocation of your trained model file\n\n\n\n\n\n\nnum_sample_images ¬∑ default: 5\n\nNumber of drawings to analyse (max 5)\n\ncontrast_values ¬∑ default: [0.5, 0.75, 1, 1.5, 2, 3]\n\nContrast levels to test\n\n\n\n\n\n\nPatch visualization diagrams\nContrast effect comparisons\nImage summary statistics\nProcessing recommendations\n\n\n\n\nRun analysis on a new dataset:\nrun_diagnostics(\n    \"new_site_drawings/\",\n    model_path=\"model_601.pkl\",\n    contrast_values=[0.75, 1, 1.25, 1.5]\n)",
    "crumbs": [
      "API Documentation",
      "ink"
    ]
  },
  {
    "objectID": "api_ink.html#calculate-patches",
    "href": "api_ink.html#calculate-patches",
    "title": "Ink Module",
    "section": "",
    "text": "def calculate_patches(\n    width: int,\n    height: int,\n    patch_size: int = 512,\n    overlap: int = 64\n) -&gt; tuple[int, int, int]\nInternal utility that determines optimal patch division for processing large drawings. Ensures efficient memory usage while maintaining detail preservation.\n\n\n\nwidth, height\n\nImage dimensions in pixels\n\npatch_size ¬∑ default: 512\n\nSize of processing segments\n\noverlap ¬∑ default: 64\n\nOverlap between segments\n\n\n\n\n\nReturns a tuple containing:\n\ntotal_patches: Total number of segments\npatches_per_row: Number of patches horizontally\nnum_rows: Number of patches vertically\n\n\n\n\nCalculate processing segments:\npatches, rows, cols = calculate_patches(2048, 1536)\nprint(f\"Processing in {patches} segments\")",
    "crumbs": [
      "API Documentation",
      "ink"
    ]
  },
  {
    "objectID": "api_preprocessing.html",
    "href": "api_preprocessing.html",
    "title": "Preprocessing Module",
    "section": "",
    "text": "The preprocessing module provides tools for analyzing and adjusting archaeological drawings before conversion. It ensures optimal input quality through statistical analysis and targeted adjustments.\n\n\nclass DatasetAnalyzer:\n    def __init__(self):\n        self.metrics = {}\n        self.distributions = {}\nA comprehensive tool for analysing collections of archaeological drawings, establishing statistical baselines for quality control.\n\n\n\n\ndef analyze_image(self, image: Union[str, Image.Image]) -&gt; dict\nExtracts key metrics from a single drawing.\nReturns\n\nmean: Average brightness\nstd: Standard deviation\ncontrast_ratio: Dynamic range measure\nmedian: Middle intensity value\ndynamic_range: Total intensity range\nentropy: Image information content\niqr: Inter-quartile range\nnon_empty_ratio: Drawing density measure\n\n\n\n\ndef analyze_dataset(\n    self, \n    dataset_path: str,\n    file_pattern: tuple = ('.png', '.jpg', '.jpeg')\n) -&gt; dict\nBuilds statistical distributions from a collection of drawings.\n\n\n\ndef visualize_distributions_kde(\n    self,\n    metrics_to_plot: Optional[List[str]] = None,\n    save: bool = False\n)\nCreates KDE plots of metric distributions with statistical annotations.\n\n\n\ndef save_analysis(self, path: str) -&gt; None\nSaves the current analysis results to a file for later use. This is particularly useful when establishing reference metrics for a specific archaeological context or drawing style.\n\n\n\npath\n\nFile path to save the analysis results\n\n\n\n\n\nanalyzer = DatasetAnalyzer()\nstats = analyzer.analyze_dataset(\"reference_drawings/\")\nanalyzer.save_analysis(\"reference_metrics.npy\")\n\n\n\n\n@classmethod\ndef load_analysis(cls, path: str) -&gt; 'DatasetAnalyzer'\nClass method that loads previously saved analysis results. This allows reuse of established reference metrics without reanalyzing the dataset.\n\n\n\npath\n\nPath to previously saved analysis file\n\n\n\n\n\nReturns a new DatasetAnalyzer instance with loaded analysis results\n\n\n\n# Load previously computed statistics\nanalyzer = DatasetAnalyzer.load_analysis(\"reference_metrics.npy\")\n\n# Use loaded stats for quality checks\ncheck = check_image_quality(\"new_drawing.jpg\", analyzer.distributions)\nThese methods enable efficient reuse of analysis results across multiple processing sessions, particularly valuable when working with established archaeological documentation standards or specific site collections.\n\n\n\n\n\n\ndef process_folder_metrics(\n    input_folder: str,\n    model_stats: dict,\n    file_extensions: tuple = ('.jpg', '.jpeg', '.png')\n) -&gt; None\nBatch processes a folder of drawings to align their metrics with reference statistics.\n\n\n\ninput_folder\n\nDirectory containing drawings to process\n\nmodel_stats\n\nReference statistics from DatasetAnalyzer\n\nfile_extensions\n\nSupported file types\n\n\n\n\n\n\ndef apply_recommended_adjustments(\n    image: Union[str, Image.Image],\n    model_stats: dict,\n    verbose: bool = True\n) -&gt; Image.Image\nAutomatically adjusts a drawing based on statistical analysis.\n\n\n\nimage\n\nDrawing to adjust\n\nmodel_stats\n\nReference statistics\n\nverbose\n\nPrint adjustment details\n\n\n\n\n\n\nContrast normalization\nBrightness alignment\nStandard deviation correction\nDynamic range optimization\n\n\n\n\nadjusted = apply_recommended_adjustments(\n    \"drawing.jpg\",\n    reference_stats,\n    verbose=True\n)\n\n\n\n\ndef check_image_quality(\n    image: Union[str, Image.Image],\n    model_stats: dict\n) -&gt; dict\nEvaluates a drawing against reference metrics to identify needed adjustments.\n\n\nReturns a dictionary containing:\n\nmetrics: Current image measurements\nrecommendations: List of suggested adjustments\nis_compatible: Boolean indicating if adjustments needed\n\n\n\n\ncheck = check_image_quality(\"new_drawing.jpg\", reference_stats)\nif not check['is_compatible']:\n    print(\"Adjustments needed:\", check['recommendations'])\n\n\n\n\ndef visualize_metrics_change(\n    original_metrics: dict,\n    adjusted_metrics: dict,\n    model_stats: dict,\n    metrics_to_plot: Optional[List[str]] = None,\n    save: bool = False\n) -&gt; None\nCreates detailed visualizations comparing original and adjusted metrics against reference distributions.\n\n\n\noriginal_metrics\n\nMetrics before adjustment\n\nadjusted_metrics\n\nMetrics after adjustment\n\nmodel_stats\n\nReference statistics\n\nmetrics_to_plot\n\nSpecific metrics to visualize\n\nsave\n\nSave plot to file\n\n\n\n\n\nvisualize_metrics_change(\n    original_metrics,\n    adjusted_metrics,\n    reference_stats,\n    metrics_to_plot=['contrast_ratio', 'mean', 'std']\n)",
    "crumbs": [
      "API Documentation",
      "preprocessing"
    ]
  },
  {
    "objectID": "api_preprocessing.html#dataset-analyzer",
    "href": "api_preprocessing.html#dataset-analyzer",
    "title": "Preprocessing Module",
    "section": "",
    "text": "class DatasetAnalyzer:\n    def __init__(self):\n        self.metrics = {}\n        self.distributions = {}\nA comprehensive tool for analysing collections of archaeological drawings, establishing statistical baselines for quality control.\n\n\n\n\ndef analyze_image(self, image: Union[str, Image.Image]) -&gt; dict\nExtracts key metrics from a single drawing.\nReturns\n\nmean: Average brightness\nstd: Standard deviation\ncontrast_ratio: Dynamic range measure\nmedian: Middle intensity value\ndynamic_range: Total intensity range\nentropy: Image information content\niqr: Inter-quartile range\nnon_empty_ratio: Drawing density measure\n\n\n\n\ndef analyze_dataset(\n    self, \n    dataset_path: str,\n    file_pattern: tuple = ('.png', '.jpg', '.jpeg')\n) -&gt; dict\nBuilds statistical distributions from a collection of drawings.\n\n\n\ndef visualize_distributions_kde(\n    self,\n    metrics_to_plot: Optional[List[str]] = None,\n    save: bool = False\n)\nCreates KDE plots of metric distributions with statistical annotations.\n\n\n\ndef save_analysis(self, path: str) -&gt; None\nSaves the current analysis results to a file for later use. This is particularly useful when establishing reference metrics for a specific archaeological context or drawing style.\n\n\n\npath\n\nFile path to save the analysis results\n\n\n\n\n\nanalyzer = DatasetAnalyzer()\nstats = analyzer.analyze_dataset(\"reference_drawings/\")\nanalyzer.save_analysis(\"reference_metrics.npy\")\n\n\n\n\n@classmethod\ndef load_analysis(cls, path: str) -&gt; 'DatasetAnalyzer'\nClass method that loads previously saved analysis results. This allows reuse of established reference metrics without reanalyzing the dataset.\n\n\n\npath\n\nPath to previously saved analysis file\n\n\n\n\n\nReturns a new DatasetAnalyzer instance with loaded analysis results\n\n\n\n# Load previously computed statistics\nanalyzer = DatasetAnalyzer.load_analysis(\"reference_metrics.npy\")\n\n# Use loaded stats for quality checks\ncheck = check_image_quality(\"new_drawing.jpg\", analyzer.distributions)\nThese methods enable efficient reuse of analysis results across multiple processing sessions, particularly valuable when working with established archaeological documentation standards or specific site collections.",
    "crumbs": [
      "API Documentation",
      "preprocessing"
    ]
  },
  {
    "objectID": "api_preprocessing.html#process-folder-metrics",
    "href": "api_preprocessing.html#process-folder-metrics",
    "title": "Preprocessing Module",
    "section": "",
    "text": "def process_folder_metrics(\n    input_folder: str,\n    model_stats: dict,\n    file_extensions: tuple = ('.jpg', '.jpeg', '.png')\n) -&gt; None\nBatch processes a folder of drawings to align their metrics with reference statistics.\n\n\n\ninput_folder\n\nDirectory containing drawings to process\n\nmodel_stats\n\nReference statistics from DatasetAnalyzer\n\nfile_extensions\n\nSupported file types",
    "crumbs": [
      "API Documentation",
      "preprocessing"
    ]
  },
  {
    "objectID": "api_preprocessing.html#apply-recommended-adjustments",
    "href": "api_preprocessing.html#apply-recommended-adjustments",
    "title": "Preprocessing Module",
    "section": "",
    "text": "def apply_recommended_adjustments(\n    image: Union[str, Image.Image],\n    model_stats: dict,\n    verbose: bool = True\n) -&gt; Image.Image\nAutomatically adjusts a drawing based on statistical analysis.\n\n\n\nimage\n\nDrawing to adjust\n\nmodel_stats\n\nReference statistics\n\nverbose\n\nPrint adjustment details\n\n\n\n\n\n\nContrast normalization\nBrightness alignment\nStandard deviation correction\nDynamic range optimization\n\n\n\n\nadjusted = apply_recommended_adjustments(\n    \"drawing.jpg\",\n    reference_stats,\n    verbose=True\n)",
    "crumbs": [
      "API Documentation",
      "preprocessing"
    ]
  },
  {
    "objectID": "api_preprocessing.html#check-image-quality",
    "href": "api_preprocessing.html#check-image-quality",
    "title": "Preprocessing Module",
    "section": "",
    "text": "def check_image_quality(\n    image: Union[str, Image.Image],\n    model_stats: dict\n) -&gt; dict\nEvaluates a drawing against reference metrics to identify needed adjustments.\n\n\nReturns a dictionary containing:\n\nmetrics: Current image measurements\nrecommendations: List of suggested adjustments\nis_compatible: Boolean indicating if adjustments needed\n\n\n\n\ncheck = check_image_quality(\"new_drawing.jpg\", reference_stats)\nif not check['is_compatible']:\n    print(\"Adjustments needed:\", check['recommendations'])",
    "crumbs": [
      "API Documentation",
      "preprocessing"
    ]
  },
  {
    "objectID": "api_preprocessing.html#visualize-metrics-change",
    "href": "api_preprocessing.html#visualize-metrics-change",
    "title": "Preprocessing Module",
    "section": "",
    "text": "def visualize_metrics_change(\n    original_metrics: dict,\n    adjusted_metrics: dict,\n    model_stats: dict,\n    metrics_to_plot: Optional[List[str]] = None,\n    save: bool = False\n) -&gt; None\nCreates detailed visualizations comparing original and adjusted metrics against reference distributions.\n\n\n\noriginal_metrics\n\nMetrics before adjustment\n\nadjusted_metrics\n\nMetrics after adjustment\n\nmodel_stats\n\nReference statistics\n\nmetrics_to_plot\n\nSpecific metrics to visualize\n\nsave\n\nSave plot to file\n\n\n\n\n\nvisualize_metrics_change(\n    original_metrics,\n    adjusted_metrics,\n    reference_stats,\n    metrics_to_plot=['contrast_ratio', 'mean', 'std']\n)",
    "crumbs": [
      "API Documentation",
      "preprocessing"
    ]
  },
  {
    "objectID": "docs_main.html",
    "href": "docs_main.html",
    "title": "PyPotteryLens: first steps into automatic inking",
    "section": "",
    "text": "Introduction\nArchaeological drawing digitisation represents a crucial step in preserving and sharing our cultural heritage. While traditionally done by hand, modern computational methods can help streamline this process while maintaining the high standards required for archaeological documentation. This guide walks you through using the ink module, a specialised tool designed for converting pencil drawings of archaeological artefacts into publication-ready inked versions.\n\n\nSetting Up Your Environment\nLet‚Äôs begin by importing the necessary functions from the toolset:\nfrom ink import process_folder, run_diagnostics\nThese two functions serve as the foundation of our processing pipeline. The run_diagnostics function helps us understand how our images will interact with the model, while process_folder handles the actual conversion process.\nThink of run_diagnostics as our planning phase and process_folder as our execution phase.\n\n\nUnderstanding the Diagnostic Process\nBefore processing an entire collection of drawings, we need to understand how our specific drawings will interact with the model.\nFirst, let‚Äôs set up our working directory:\nmy_path = \"Montale_example\"\nNow we can run our diagnostic analysis:\nrun_diagnostics(\n    input_folder=my_path,           # Where your drawings are stored\n    model_path=\"6h-MCG.pkl\",        # The trained model file\n    num_sample_images=1,            # How many test images to analyze\n    contrast_values=[0.5, 0.75, 1, 1.5, 2, 3],  # Different contrast levels to test\n)\nLet‚Äôs understand each parameter in detail:\n\ninput_folder: This is where your pencil drawings are stored. The function will randomly select images from this folder for analysis.\nmodel_path: Points to your trained model file. This model contains the learned patterns for converting pencil drawings to inked versions.\nnum_sample_images: Controls how many images to analyse. For a small collection, 1-2 images might suffice, but for larger collections with varying drawing styles, you might want to test more.\ncontrast_values: These values help us understand how different contrast levels affect the model‚Äôs output. Think of this like adjusting the pressure of your pen when inking - too light (low contrast) might lose details, while too heavy (high contrast) might create unwanted artefacts.\n\nThe diagnostic process creates a structured output that helps us understand how our images will be processed:\ndiagnostic/\n‚îú‚îÄ‚îÄ contrast_analysis_{progressive_number}.png  # Shows how contrast affects results\n‚îú‚îÄ‚îÄ patches_{progressive_number}.png            # Shows how images will be divided\n‚îî‚îÄ‚îÄ summary_{progressive_number}.txt            # Contains detailed analysis\nLet‚Äôs examine the patch division output (Figure¬†1):\n\n\n\n\n\n\nFigure¬†1: This visualization shows how your image will be processed in smaller chunks. The green lines indicate patch boundaries, while yellow areas show where patches overlap to ensure smooth transitions.\n\n\n\nThe patch division is crucial because it shows us how the model will break down larger images into manageable pieces. The overlap between patches helps prevent visible seams in the final output.\nNext, let‚Äôs look at the contrast analysis (Figure¬†2):\n\n\n\n\n\n\nFigure¬†2: This analysis shows how different contrast values affect the model‚Äôs output. Notice how extreme values can either lose detail (too low) or create artefacts (too high).\n\n\n\nThis visualization is particularly important because it helps us find the sweet spot for contrast enhancement. Looking at the results:\n\nA contrast value of 0.5 is too low, causing loss of fine details\nA contrast value of 3.0 is too high, introducing unwanted artefacts\nValues between 1.0 and 1.5 seem to produce the most balanced results\n\n\n\nProcessing Your Collection\nAfter understanding how our images interact with the model through diagnostics, we can proceed with processing our entire collection:\nprocess_folder(\n    input_folder=my_path,\n    model_path=\"6h-MCG.pkl\",\n    contrast_scale=1.25,            # Chosen based on diagnostic results\n    output_dir=\"Montale_processed\", # We define an output folder\n    use_fp16=True,                  # Enables faster processing\n    upscale=1                       # Upscale the image (1: no upscaling)\n)\nThe processing creates an organized output structure that helps us track and validate our results:\nMontale_processed/\n‚îú‚îÄ‚îÄ comparisons/                # Before/after comparisons\n‚îÇ   ‚îú‚îÄ‚îÄ comparison_{image_name}.jpg\n‚îú‚îÄ‚îÄ logs/                       # Detailed processing records\n‚îÇ   ‚îî‚îÄ‚îÄ processing_log_{timestamp}.txt\n‚îú‚îÄ‚îÄ {image_name}.jpg           # Final processed images\nThe log files contain valuable information about the processing:\nProcessing started at: 2024-12-30 13:26:26.607305\nConfiguration:\n- Input folder: Montale_example\n- Output directory: Montale_processed\n- Model path: model_601.pkl\n- FP16 mode: True\n- Patch size: 512px\n- Overlap: 64px\n- Contrast scale: 1.25\n- Prompt: make it ready for publication\n\n[Processing details...]\nThe comparison images (Figure¬†3) help us verify the quality of our results:\n\n\n\n\n\n\nFigure¬†3: Side-by-side comparison showing the transformation from pencil drawing to inked version. Notice how the model maintains detail while creating clean, publication-ready lines.\n\n\n\n\n\nBest Practices and Troubleshooting\nTo get the best results from your processing:\n\nAlways start with diagnostics, even if you‚Äôre familiar with the model. Different collections might require slightly different parameters.\nPay attention to image resolution. The model works best with clear, well-scanned drawings. If your scans are too light or too dark, consider adjusting them before processing (see preprocessing).\nMonitor the processing logs. If you see many failed conversions, it might indicate issues with your input images or parameters.\nReview the comparison images carefully. They can help you spot any systematic issues that might need addressing.\nUpscaling can help achieve a better result, especially when dealing with complex decorations. Upscaling factor is just for processing. The upscaling factor is for processing only. It doesn‚Äôt affect the output size.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that while this tool automates much of the inking process, it‚Äôs still important to review the results with an archaeological perspective. The model is a tool to assist, not replace, archaeological expertise.",
    "crumbs": [
      "Documentation",
      "Main Processing"
    ]
  },
  {
    "objectID": "docs_preprocessing.html",
    "href": "docs_preprocessing.html",
    "title": "PyPotteryLens: Preprocessing Pipeline Documentation",
    "section": "",
    "text": "Introduction to Image Preprocessing\nArchaeological drawings often vary in their characteristics due to different artists, scanning conditions, and original materials. These variations can impact the performance of the model. This guide outlines the preprocessing pipeline that helps standardise images before they are processed by the model.\n\n\nImport Required Libraries\n\nfrom ink import process_folder, process_single_image\nfrom preprocessing import DatasetAnalyzer, process_image_metrics, \n                visualize_metrics_change, process_folder_metrics\n\n\n\nUnderstanding the Problem: Image Metric Variations\nLet‚Äôs examine a specific case that illustrates why preprocessing is necessary. Consider the following high-contrast image (Figure¬†1).\n\n\n\n\n\n\nFigure¬†1: An archaeological drawing with excessive contrast. The high contrast is evident in the black areas\n\n\n\nWhen we apply our model directly to this image without preprocessing, we get problematic results (Figure¬†2):\n\n\n\n\n\n\nFigure¬†2: Direct model application showing artefacts. Note the missing dotting patterns and blue tiling artefacts in the bottom-right region.\n\n\n\nThe output shows two significant issues:\n\nMissing dot patterns in shaded areas\nUnwanted blue tiling artefacts, particularly visible in the bottom-right section\n\nThese issues arise because the input image‚Äôs characteristics deviate significantly from the training dataset‚Äôs standard properties. Our preprocessing pipeline addresses this by analysing and adjusting key image metrics to align them with our training data distribution.\n\n\nImage Metrics Analysis\nThe DatasetAnalyzer class calculates eight key metrics that characterise archaeological drawings:\n\nMean brightness: Overall lightness of the image\nStandard deviation: Measure of tonal variation\nContrast ratio: Ratio between the brightest and darkest areas\nMedian: Middle intensity value\nDynamic range: Difference between the 99th and 1st percentile intensities\nEntropy: Measure of information content\nIQR (Interquartile Range): Spread of middle 50% of intensity values\nNon-empty ratio: Proportion of pixels above background threshold\n\nYou can either analyse your own dataset to establish reference metrics or use our pre-computed metrics from the training dataset:\n\n# Option 1: Analyse your own dataset\nanalyzer = DatasetAnalyzer()\nmodel_stats = analyzer.analyze_dataset('PATH_TO_YOUR_DATASET')\nanalyzer.save_analysis('DATASET_RESULTS_PATH')\n\n# Option 2: Use pre-computed metrics\nanalyzer = DatasetAnalyzer()\nanalyzer = analyzer.load_analysis('Montale_stats.npy')\nmodel_stats = analyzer.distributions\n\nThe distribution of these metrics can be visualized:\n\nanalyzer.visualize_distributions_kde()\n\n\n\n\n\n\n\nFigure¬†3: Distribution of image metrics from the training dataset. Each plot shows the distribution curve, actual data points, mean (red dashed line), median (blue dotted line), and ¬±2œÉ range (green shading).\n\n\n\n\n\nPreprocessing Pipeline\nThe preprocessing pipeline adjusts images to match the statistical properties of the training dataset. Here‚Äôs how to process a single image:\n\noriginal_image = 'PATH_TO_YOUR_IMAGE'\nadjusted_image, adjusted_metrics = process_image_metrics(original_image, model_stats)\n\nThe pipeline performs these adjustments:\n\nContrast normalization based on the training set‚Äôs contrast ratio distribution\nDynamic range adjustment to match the target range\nBrightness correction to align with the training set‚Äôs mean intensity\n\nThe adjusted image shows more balanced contrast and better preservation of details:\n\n\n\n\n\n\nFigure¬†4: Preprocessed image showing normalized contrast and preserved details.\n\n\n\nWe can visualize how the preprocessing affected our image metrics:\n\n\n\n\n\n\nFigure¬†5: Comparison of original (orange) and adjusted (teal) metrics against the training distribution.\n\n\n\n\n\nModel Application\nAfter preprocessing, the model produces significantly better results:\n\n\n\n\n\n\nFigure¬†6: Model output after preprocessing, showing proper dotting patterns and no artifacts.\n\n\n\nThe improvements include:\n\nConsistent dotting patterns in shaded areas\nNo colour artefacts or tiling effects\nBetter preservation of fine details\nMore natural transition between different tonal areas\n\nI‚Äôll add a section about batch processing to the documentation:\n\n\nBatch Processing\nWhile processing individual images is useful for testing and fine-tuning parameters, in archaeological practice we often need to process entire collections of drawings. The process_folder_metrics function automates this task by applying our preprocessing pipeline to all images in a directory.\nHere‚Äôs how to use batch processing:\n# Define the input folder containing your drawings\ninput_folder = \"path/to/your/drawings\"\n\n# Process all images in the folder\nprocess_folder_metrics(\n    input_folder=input_folder,\n    model_stats=model_stats,\n    file_extensions=('.jpg', '.jpeg', '.png')  # Supported file formats\n)\nThe function creates a new directory named input_folder_adjusted that contains all the processed images. During processing, it provides detailed feedback about each image:\nüìÅ Found 25 images to process\n==================================================\n\nüîç Analyzing: drawing_001.jpg\n‚öôÔ∏è Image Analysis:\n‚îî‚îÄ Contrast: 0.85x | Brightness adjustment needed\n‚ú® Adjustments applied\n\nProgress: 1/25\n--------------------------------------------------\n\nüîç Analyzing: drawing_002.jpg\n‚úÖ Image metrics within normal ranges\n\nProgress: 2/25\n--------------------------------------------------\nAt the end of processing, you‚Äôll receive a summary report:\nüìä Processing Summary\n==================================================\nTotal processed:  25\nImages adjusted:  18\nNo adjustments:   7\n\nüíæ All images saved to: path/to/your/drawings_adjusted\nThis summary helps you understand how many images required adjustment, which can be useful for:\n\nIdentifying systematic issues in your drawing or scanning process\nPlanning future preprocessing strategies\nDocumenting the processing pipeline for publication\n\nThe batch processing maintains all the benefits of individual processing while saving time and ensuring consistency across your entire dataset. All adjusted images will have metrics that align with your training data distribution, leading to better results when applying the model.\nNote that each image still receives individual analysis and custom adjustments - the batch process doesn‚Äôt apply a one-size-fits-all transformation. Instead, it analyses each drawing‚Äôs specific characteristics and applies the precise adjustments needed to bring that particular image into alignment with your target metrics.",
    "crumbs": [
      "Documentation",
      "Preprocessing"
    ]
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation Guide",
    "section": "",
    "text": "PyPotteryInk requires specific hardware and software components to function properly. This guide will walk you through the installation process step by step, ensuring you have everything needed to start processing archaeological drawings.\n\n\nBefore installing PyPotteryInk, ensure your system meets these minimum requirements:\n\n\nFor model inference (using the tool):\n\nCPU: Modern multi-core processor (Intel i5/i7 or AMD Ryzen 5/7)\nRAM: 16GB minimum, 32GB recommended\nStorage: Minimum 10GB for the base installation and models\nGPU: NVIDIA GPU with 8GB VRAM (for optimal performance)\n\nFor model training or fine-tuning:\n\nGPU: NVIDIA GPU with at least 24GB VRAM\nRAM: 32GB minimum\nStorage: 100GB+ recommended for datasets and model checkpoints\n\n\n\n\nPyPotteryInk requires Python 3.10 or newer and depends on several deep learning frameworks:\n\nCUDA Toolkit 11.8 or newer (for GPU support)\nPyTorch 2.0 or newer\nOther Python dependencies (automatically installed)\n\n\n\n\n\n\n\nFirst, download the latest release from our GitHub repository. You can find the release package at:\n\n  \n\n\n\n\nPyPotteryInk includes a unified installation script that works on all platforms:\ngit clone https://github.com/lrncrd/PyPotteryInk.git\ncd PyPotteryInk\npython install.py\nThe installation script will: - Create a virtual environment\n- Install all dependencies\n- Download required models\n- Set up the application\n\n\nAfter installation:\n- Windows: Double-click run.bat or run it from terminal\n- macOS/Linux: Run ./run.sh or python app.py\nThe web interface will open automatically in your browser at http://127.0.0.1:7860 (or the next available port).\n\n\n\n\nAfter downloading, extract the package to your desired location. The folder structure should look like this:\npypotteryink/\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ models/ # This folder will be created by running test.py\n‚îú‚îÄ‚îÄ ink.py\n‚îú‚îÄ‚îÄ models.py\n‚îú‚îÄ‚îÄ preprocessing.py\n‚îú‚îÄ‚îÄ postprocessing.py\n‚îú‚îÄ‚îÄ test.py\n‚îú‚îÄ‚îÄ utils.py\n‚îî‚îÄ‚îÄ Montale_stats.npy\n\n\nMove to the downloaded folder and run this command\npip install -r requirements.txt\nThe installation might take several minutes depending on your internet connection.\n\n\n\nAfter installation, you can verify everything is working correctly. Move into the folder and run\npython test.py\nThe test will detect CUDA installation and run some functions to a test image.\n\n\n\n\n\n\n\nIf you see an error about CUDA not being available:\n\nEnsure you have the NVIDIA drivers installed\nVerify CUDA installation by running:\n\nnvidia-smi\n\nMake sure your GPU is CUDA-capable and properly recognised\n\n\n\n\n\n\n\nImportant\n\n\n\nThis guide is about inference. For training or fine-tuning, check out the Training guide section!",
    "crumbs": [
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#system-requirements",
    "href": "installation.html#system-requirements",
    "title": "Installation Guide",
    "section": "",
    "text": "Before installing PyPotteryInk, ensure your system meets these minimum requirements:\n\n\nFor model inference (using the tool):\n\nCPU: Modern multi-core processor (Intel i5/i7 or AMD Ryzen 5/7)\nRAM: 16GB minimum, 32GB recommended\nStorage: Minimum 10GB for the base installation and models\nGPU: NVIDIA GPU with 8GB VRAM (for optimal performance)\n\nFor model training or fine-tuning:\n\nGPU: NVIDIA GPU with at least 24GB VRAM\nRAM: 32GB minimum\nStorage: 100GB+ recommended for datasets and model checkpoints\n\n\n\n\nPyPotteryInk requires Python 3.10 or newer and depends on several deep learning frameworks:\n\nCUDA Toolkit 11.8 or newer (for GPU support)\nPyTorch 2.0 or newer\nOther Python dependencies (automatically installed)",
    "crumbs": [
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#installation-steps",
    "href": "installation.html#installation-steps",
    "title": "Installation Guide",
    "section": "",
    "text": "First, download the latest release from our GitHub repository. You can find the release package at:\n\n  \n\n\n\n\nPyPotteryInk includes a unified installation script that works on all platforms:\ngit clone https://github.com/lrncrd/PyPotteryInk.git\ncd PyPotteryInk\npython install.py\nThe installation script will: - Create a virtual environment\n- Install all dependencies\n- Download required models\n- Set up the application\n\n\nAfter installation:\n- Windows: Double-click run.bat or run it from terminal\n- macOS/Linux: Run ./run.sh or python app.py\nThe web interface will open automatically in your browser at http://127.0.0.1:7860 (or the next available port).\n\n\n\n\nAfter downloading, extract the package to your desired location. The folder structure should look like this:\npypotteryink/\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ models/ # This folder will be created by running test.py\n‚îú‚îÄ‚îÄ ink.py\n‚îú‚îÄ‚îÄ models.py\n‚îú‚îÄ‚îÄ preprocessing.py\n‚îú‚îÄ‚îÄ postprocessing.py\n‚îú‚îÄ‚îÄ test.py\n‚îú‚îÄ‚îÄ utils.py\n‚îî‚îÄ‚îÄ Montale_stats.npy\n\n\nMove to the downloaded folder and run this command\npip install -r requirements.txt\nThe installation might take several minutes depending on your internet connection.\n\n\n\nAfter installation, you can verify everything is working correctly. Move into the folder and run\npython test.py\nThe test will detect CUDA installation and run some functions to a test image.",
    "crumbs": [
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#common-installation-issues",
    "href": "installation.html#common-installation-issues",
    "title": "Installation Guide",
    "section": "",
    "text": "If you see an error about CUDA not being available:\n\nEnsure you have the NVIDIA drivers installed\nVerify CUDA installation by running:\n\nnvidia-smi\n\nMake sure your GPU is CUDA-capable and properly recognised\n\n\n\n\n\n\n\nImportant\n\n\n\nThis guide is about inference. For training or fine-tuning, check out the Training guide section!",
    "crumbs": [
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Guide to Fine-tuning PyPotteryInk Models",
    "section": "",
    "text": "This guide walks you through the process of fine-tuning a PyPotteryInk model for your specific archaeological context.",
    "crumbs": [
      "Documentation",
      "Training Guide"
    ]
  },
  {
    "objectID": "training.html#prerequisites",
    "href": "training.html#prerequisites",
    "title": "Guide to Fine-tuning PyPotteryInk Models",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting the fine-tuning process, ensure you have:\n\nA GPU with at least 20GB VRAM for training\nPython 3.10 or higher\nA paired dataset of pencil drawings and their inked versions\nStorage space for model checkpoints and training data",
    "crumbs": [
      "Documentation",
      "Training Guide"
    ]
  },
  {
    "objectID": "training.html#environment-setup",
    "href": "training.html#environment-setup",
    "title": "Guide to Fine-tuning PyPotteryInk Models",
    "section": "Environment Setup",
    "text": "Environment Setup\n\nFirst, clone the repository:\n\ngit clone https://github.com/GaParmar/img2img-turbo.git\n\nInstall the required dependencies:\n\npip install -r img2img-turbo/requirements.txt\npip install git+https://github.com/openai/CLIP.git\npip install wandb vision_aided_loss huggingface-hub==0.25.0",
    "crumbs": [
      "Documentation",
      "Training Guide"
    ]
  },
  {
    "objectID": "training.html#dataset-preparation",
    "href": "training.html#dataset-preparation",
    "title": "Guide to Fine-tuning PyPotteryInk Models",
    "section": "Dataset Preparation",
    "text": "Dataset Preparation\n\nTo create a training dataset check out the original docs (https://github.com/GaParmar/img2img-turbo/blob/main/docs/training_pix2pix_turbo.md)\nImportant considerations for dataset preparation:\n\nImages should be paired (same filename in both folders)\nStandard image formats (jpg, jpeg, png) are supported\nBoth pencil and inked versions should be aligned\nRecommended resolution: at least 512x512 pixels\n\nData requirements:\n\nMinimum recommended: 10-20 pairs for fine-tuning\nEach drawing should be clean and well-scanned\nInclude variety in pottery types and decorations\nConsistent drawing style across the dataset",
    "crumbs": [
      "Documentation",
      "Training Guide"
    ]
  },
  {
    "objectID": "training.html#setting-up-fine-tuning",
    "href": "training.html#setting-up-fine-tuning",
    "title": "Guide to Fine-tuning PyPotteryInk Models",
    "section": "Setting Up Fine-tuning",
    "text": "Setting Up Fine-tuning\nTo fine-tune a pre-trained model (like ‚Äú6h-MCG‚Äù), you‚Äôll need to modify the base img2img-turbo repository. This enables the use of a pre-trained model as a starting point for your specialized training.\n\nPrepare the Repository:\n\nNavigate to your cloned img2img-turbo directory\n\ncd img2img-turbo\nReplace Key Files:\n\nCopy these files from the PyPotteryInk repository‚Äôs ‚Äúfine-tuning‚Äù folder into the src folder:\n\npix2pix_turbo.py\ntrain_pix2pix_turbo.py\n\n\nThese modified files contain the necessary adaptations to support training from a pre-trained model.",
    "crumbs": [
      "Documentation",
      "Training Guide"
    ]
  },
  {
    "objectID": "training.html#running-fine-tuning",
    "href": "training.html#running-fine-tuning",
    "title": "Guide to Fine-tuning PyPotteryInk Models",
    "section": "Running Fine-tuning",
    "text": "Running Fine-tuning\n\nInitialize Accelerate Environment:\naccelerate config\nThis will guide you through setting up your training environment. Follow the prompts to configure for your GPU setup.\nStart Training:\naccelerate launch src/train_pix2pix_turbo.py \\\n    --pretrained_model_name_or_path=\"6h-MCG.pkl\" \\\n    --output_dir=\"YOUR_OUTPUT_DIR\" \\\n    --dataset_folder=\"YOUR_INPUT_DATA\" \\\n    --resolution=512 \\\n    --train_batch_size=2 \\\n    --enable_xformers_memory_efficient_attention \\\n    --viz_freq 25 \\\n    --track_val_fid \\\n    --report_to \"wandb\" \\\n    --tracker_project_name \"YOUR_PROJECT_NAME\"\nKey Parameters:\n\npretrained_model_name_or_path: Path to your pre-trained model (e.g., ‚Äú6h-MCG.pkl‚Äù)\noutput_dir: Where to save training outputs and checkpoints\ndataset_folder: Location of your training dataset\nresolution: Image resolution (512 recommended)\ntrain_batch_size: Number of images per training batch\nviz_freq: How often to generate visualization samples\ntrack_val_fid: Enable FID score tracking\ntracker_project_name: Your Weights & Biases project name\n\n\nNote: Adjust the batch size based on your GPU memory. Start with 2 and increase if your GPU can handle it.",
    "crumbs": [
      "Documentation",
      "Training Guide"
    ]
  },
  {
    "objectID": "training.html#important-considerations",
    "href": "training.html#important-considerations",
    "title": "Guide to Fine-tuning PyPotteryInk Models",
    "section": "Important Considerations",
    "text": "Important Considerations\n\nEnsure your pre-trained model file (e.g., ‚Äú6h-MCG.pkl‚Äù) is in the correct location\nMonitor GPU memory usage during training\nUse Weights & Biases (wandb) to track training progress\nCheck the output directory periodically for sample outputs\nTraining time will vary based on your GPU and dataset size",
    "crumbs": [
      "Documentation",
      "Training Guide"
    ]
  }
]